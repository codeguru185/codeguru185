from pyspark.sql import SparkSession
from pyspark.sql.functions import col, when
from pyspark.sql.types import DateType, TimestampType, DoubleType

# Create a Spark session
spark = SparkSession.builder.appName("FieldNamesCategorization").getOrCreate()

# Sample DataFrame (Replace this with your own DataFrame)
data = [
    ('A', 'X', '2022-01-01', '2022-01-01 12:00:00', 100, 50, 5, 1.2, 1.234),
    ('B', 'Y', '2022-01-02', '2022-01-02 14:30:00', 200, 75, 10, 2.5, 2.345),
    ('A', 'Z', '2022-01-03', '2022-01-03 10:15:00', 300, 120, 15, 3.8, 3.456),
    ('C', 'X', '2022-01-04', '2022-01-04 16:45:00', 400, 80, 20, 4.0, 4.567),
    ('B', 'Y', '2022-01-05', '2022-01-05 08:30:00', 500, 90, 25, 5.5, 5.678)
]

columns = ["Category1", "Category2", "Date1", "Timestamp1", "Amount1", "Amount2", "Numeric1", "Numeric2", "Decimal1"]

df = spark.createDataFrame(data, columns)

# Define a function to determine data type
def infer_data_type(column):
    return when(col(column).cast(DateType()).isNotNull(), "Date") \
        .when(col(column).cast(TimestampType()).isNotNull(), "Timestamp") \
        .when(col(column).cast(DoubleType()).isNotNull(), "Double") \
        .when(col(column).cast("string").rlike(r'^\d+\.\d{2}$'), "Amount") \
        .otherwise("Other")

# Categorize columns based on data type
for column in df.columns:
    df = df.withColumn(column + "_Type", infer_data_type(column))

# Drop the original data type columns
df = df.drop(*df.columns[-len(columns):])

# Show the resulting DataFrame
df.show()
