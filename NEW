from pyspark.sql import SparkSession

# Create a Spark session
spark = SparkSession.builder.appName("FieldNamesCategorization").getOrCreate()

# Sample DataFrame (Replace this with your own DataFrame)
data = [
    ('A', 'X', 100, 50, 5, 1.2, '2022-01-01', '2022-01-01 12:00:00'),
    ('B', 'Y', 200, 75, 10, 2.5, '2022-01-02', '2022-01-02 14:30:00'),
    ('A', 'Z', 300, 120, 15, 3.8, '2022-01-03', '2022-01-03 10:15:00'),
    ('C', 'X', 400, 80, 20, 4.0, '2022-01-04', '2022-01-04 16:45:00'),
    ('B', 'Y', 500, 90, 25, 5.5, '2022-01-05', '2022-01-05 08:30:00')
]

columns = ["Category1", "Category2", "Amount1", "Amount2", "Numeric1", "Numeric2", "Date1", "Timestamp1"]

df = spark.createDataFrame(data, columns)

# Categorize columns based on criteria
categorical_less_than_20 = [col for col in df.columns if df.select(col).distinct().count() < 20 and col != "Amount1" and col != "Amount2"]
categorical_more_than_20 = [col for col in df.columns if df.select(col).distinct().count() >= 20 and col != "Amount1" and col != "Amount2"]
amount_variables = ["Amount1", "Amount2"]
date_timestamp_variables = [col for col in df.columns if df.schema[col].dataType.simpleString() in ["date", "timestamp"]]
general_numerical_variables = [col for col in df.columns if col not in categorical_less_than_20 and col not in categorical_more_than_20 and col not in amount_variables and col not in date_timestamp_variables]

# Define custom categories based on your feature engineering needs
# Replace 'CustomCategory' with meaningful names for other categories
custom_category = ["Numeric1", "Numeric2"]

# Save field names to CSV files
categories = {
    "Categorical_Less_Than_20.csv": categorical_less_than_20,
    "Categorical_More_Than_20.csv": categorical_more_than_20,
    "Amount_Variables.csv": amount_variables,
    "Date_Timestamp_Variables.csv": date_timestamp_variables,
    "General_Numerical_Variables.csv": general_numerical_variables,
    "Custom_Category.csv": custom_category
}

for filename, field_names in categories.items():
    with open(filename, "w") as file:
        file.write("\n".join(field_names)

# Stop the Spark session
spark.stop()
